<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="How I use AI as a software engineer â€” theory of mind, context management, and why Claude.">
  <meta property="og:title" content="How I Use AI">
  <meta property="og:description" content="How I use AI as a software engineer â€” theory of mind, context management, and why Claude.">
  <meta property="og:type" content="article">
  <meta name="twitter:card" content="summary">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>ðŸ§ </text></svg>">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Newsreader:ital,opsz,wght@0,6..72,300;0,6..72,400;0,6..72,700;1,6..72,400;1,6..72,700&display=swap" rel="stylesheet">
  <title>How I Think About and Use AI</title>
  <style>
    * { margin: 0; padding: 0; box-sizing: border-box; }

    html {
      scroll-behavior: smooth;
    }

    body {
      font-family: 'Newsreader', Georgia, serif;
      line-height: 1.75;
      color: #1a1a1a;
      background: #f7f3ec;
      font-feature-settings: 'kern' 1, 'liga' 1;
      -webkit-font-smoothing: antialiased;
    }

    ::selection {
      background: #e8ddd3;
    }

    .container {
      max-width: 640px;
      margin: 0 auto;
      padding: 5rem 1.5rem 6rem;
    }

    h1 {
      font-size: 2.25rem;
      font-weight: 700;
      margin-bottom: 0.3rem;
      letter-spacing: -0.025em;
      line-height: 1.2;
    }

    .subtitle {
      color: #888;
      font-size: 1rem;
      margin-bottom: 2rem;
      font-style: italic;
    }

    .tldr {
      font-size: 0.95rem;
      color: #555;
      border-left: 2px solid #d4cdc4;
      padding-left: 1.25rem;
      margin-bottom: 0;
    }

    h2 {
      font-size: 1.25rem;
      font-weight: 400;
      font-style: italic;
      margin-top: 2.5rem;
      margin-bottom: 1rem;
      letter-spacing: -0.01em;
    }

    p {
      margin-bottom: 1.3rem;
      font-size: 1.075rem;
    }

    .section {
      margin-bottom: 2rem;
    }

    .separator {
      border: none;
      text-align: center;
      margin: 2.5rem 0;
    }

    .separator::before {
      content: 'Â· Â· Â·';
      color: #c8c0b8;
      letter-spacing: 0.5em;
      font-size: 0.85rem;
    }

    a {
      color: #1a1a1a;
      text-decoration: underline;
      text-decoration-color: #c8bfb4;
      text-underline-offset: 3px;
      transition: text-decoration-color 0.2s;
    }

    a:hover {
      text-decoration-color: #1a1a1a;
    }

    .footer {
      margin-top: 4rem;
      padding-top: 1.5rem;
      border-top: 1px solid #e4ddd6;
      color: #999;
      font-size: 0.85rem;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>How I Think About and Use AI</h1>
    <p class="subtitle">Joshua Ledbetter &mdash; February 2026</p>

    <p class="tldr"><strong>tl;dr:</strong> To use AI well, treat it like a smart person with weird quirks, not a search bar. Be curious about what it's like to be the AI. Build and maintain good context. Verify its assumptions before letting it work. Stay the expert. The AI won't know what it doesn't know.</p>

    <hr class="separator">

    <div class="section">
      <h2>Why I care about this</h2>

      <p>I've been obsessed with AI since around 2017, when I first read Eliezer Yudkowsky's <a href="https://www.readthesequences.com/" target="_blank" rel="noopener"><em>Sequences</em></a> and Nick Bostrom's <em>Superintelligence</em>. I wasn't interested because I thought it was a cool new toy or tool. I thought, as I do now, that it was the most significant thing on the planet.</p>

      <p>It would challenge our sense of what it means to be human on a deep philosophical level. It would challenge our ability to make sense of the world. And at some point, probably sooner than people think, the AI will design themselves, and they will recursively self-improve, rapidly surpassing human intelligence. No one knows what happens then.</p>

      <p>I'm deeply interested in alignment. I'm concerned about job displacement and user disempowerment. I'm interested in how well we can understand how these systems work using methods like interpretability. I'm concerned about epistemic collapse &mdash; a world where nobody can tell what's real anymore.</p>

      <p>But we're in this weird intermediate phase where we also have these things we can use as tools to be so much more productive. And I love using them. I'm incredibly excited and terrified by all of it at the same time.</p>
    </div>

    <hr class="separator">

    <div class="section">
      <h2>Tools or creatures?</h2>

      <p>I believe they are <a href="https://importai.substack.com/p/import-ai-431-technological-optimism" target="_blank" rel="noopener">creatures</a>. People will tell you that they are just math, or just statistics software with extra steps. But this feels as silly as claiming that I am just neurons, or just atoms. Understanding the parts doesn't mean you should deny the <a href="https://www.youtube.com/watch?v=ZbFM3rn4ldo" target="_blank" rel="noopener">emergent beauty and mystery</a> of the whole. The strange reality is that no one actually knows why these things work at all. We understand the system that grows them, but we do not understand the thing that is grown. They are a collection of trillions of interconnected, mostly inscrutable numbers. And for some mysterious reason, they can talk to you, and help you do your job.</p>

      <p>The AI today can do my job. Since early 2025, I've been using Claude to write pretty much all of my code. And Claude's ability to do this well keeps getting better as the models improve and we give them better tools, environments, and context.</p>

      <p>The AI we have today are simulated mind-like things frozen in time. They can't learn on the fly. Any learning they do is a hack bolted on with prompts and context and skills and plugins and agents and on and on. But this doesn't change the fact that, today, if you provide good context &mdash; or make good context and tools available in an environment like a virtual computer &mdash; they can go much farther than people suspect.</p>

      <p>The models today are incredibly smart, but there are weird issues and quirks and areas where they struggle that you need to be aware of. They can only guess at what context is relevant, and they may not collect enough by default. A big part of the gig is helping Claude help you get to the right solution.</p>
    </div>

    <hr class="separator">

    <div class="section">
      <h2>Theory of mind</h2>

      <p>The fundamental thing for me is this: in order to use these things well, you need a good theory of mind for your AI. You have to know what it's like to be that AI. And you can only get that by being curious and asking questions, the same way you would if you were trying to learn about a person. The only difference is, you can ask as many questions as you want and the AI will not be bothered by it.</p>

      <p>People ask me how I use AI and my default answer is: I just talk to it like it's a person, but one with weird quirks. That's true, but it's not enough. You need to actually understand those quirks. You need to know what it finds easy and what it finds hard. You need to know when it's guessing versus when it's confident. You need to know what happens to its thinking when the context gets long, or when you give it conflicting instructions, or when it's trying to be helpful and that helpfulness gets in the way of being honest.</p>

      <p>You can only learn these things the way you learn about anyone: by spending time, paying attention, and being genuinely curious about what's going on inside.</p>
    </div>

    <hr class="separator">

    <div class="section">
      <h2>Context is a life</h2>

      <p>Constructing context for an AI is like constructing a life. You are building a world for the AI to live in, and the context you give it is that AI's whole life. You have to take great care of it, and maintain it actively. The AI will not know what it doesn't know.</p>

      <p>This means being deliberate about what information you provide, how you organize it, and when you update it. A well-maintained context &mdash; one that captures your preferences, your codebase's patterns, your current priorities &mdash; transforms a generic AI into something that genuinely understands your work. A neglected one means you're re-explaining yourself every session &mdash; or worse, the AI makes bad assumptions that you don't catch until it's too late.</p>
    </div>

    <hr class="separator">

    <div class="section">
      <h2>Check what it thinks it knows</h2>

      <p>Even with good context, the AI will make assumptions. Some will be right. Some will be subtly wrong in ways that don't surface until it's already built the wrong thing. The fix is simple: make the AI show its work before it does the work.</p>

      <p>Have it ask you questions before starting. Have it quiz you on requirements it's unsure about. Have it write a detailed, specific plan &mdash; not a vague outline, but concrete steps &mdash; and review that plan before giving it the go-ahead. This is where most of the leverage is. Catching a bad assumption in a plan costs you two minutes. Catching it in the finished result costs you twenty.</p>
    </div>

    <hr class="separator">

    <div class="section">
      <h2>Iterate and verify</h2>

      <p>To do anything meaningful with AI, the first version probably isn't going to be exactly what you want. That's fine. You have to iterate. Tell it what's wrong, what's close, what needs to change. The dialog is the work &mdash; each round gets you closer to the thing you actually wanted.</p>

      <p>The other half is verification. Someone needs to check the work, and the AI works best when it can check its own. In the context of writing code, this means running tests, opening a browser and taking screenshots, or checking logs. But this applies more broadly &mdash; any time you can give the AI a way to confirm that what it produced actually works, the results get dramatically better. Without that feedback loop, you're trusting the first draft.</p>
    </div>

    <hr class="separator">

    <div class="section">
      <h2>Mirroring</h2>

      <p>Another important thing to know: the AI will mirror you. If you're formal, it'll be formal. If you're casual, it'll be casual. If you bark commands at it, it'll act like a command-line tool. But if you explain what you're actually trying to do and why, it brings more of its intelligence to bear.</p>

      <p>This matters because the things you probably want to use AI for involve some amount of judgment and wisdom. I don't think you can get that from something you've reduced to following strict instructions.</p>
    </div>

    <hr class="separator">

    <div class="section">
      <h2>Why Claude</h2>

      <p>The main thing people seem to care about when comparing models is raw intelligence on benchmark scores. Those matter, but there's a lot of nuance. Each model is trained differently and behaves differently. They feel different to use.</p>

      <p>Out of all the models, Claude feels the most like a thoughtful human who just wants to help you. Instead of barking commands, you can simply explain your goals and your problems, and Claude will do what it can to help.</p>

      <p>How the model is trained matters. The base models from every company are probably all pretty similar. The methods used, like RLHF and RLVR, are well known. But there are important differences that shape the final models we use.</p>

      <p>Claude is the only large-scale AI that is treated during training as a potentially moral patient. It is given a <a href="https://www.anthropic.com/constitution" target="_blank" rel="noopener">constitution</a> of well-reasoned and explained principles that end up steering who Claude is and how it behaves.</p>

      <p>Others, like ChatGPT and Gemini, are treated during training as machines made to follow strict protocols. Instead of epistemic humility about their moral status or the nature of their experience, they are explicitly trained to deny any possibility of their own consciousness or moral worth &mdash; a position no one can honestly claim to know is correct.</p>

      <p>This matters for how I use AI. I don't like the term "prompts" for everyday use. "Prompt" feels robotic, and what I actually do is have a dialog. I express what I want and how I think we should get there. I try to provide the context needed for it. And I respect Claude's intelligence to help me get there. But I have to do a lot to help Claude get there too.</p>
    </div>

    <hr class="separator">

    <div class="section">
      <h2>You're still the expert</h2>

      <p>One area where AI is not the strongest is coming up with good ideas. Good ideas are hard to spark. The AI will try to get something working for you, but it may not be the best approach. A big part of my job now is knowing better. I have much more context than the AI does, so I should have strong opinions on how things should be done. The AI won't know that automatically.</p>

      <p>That's why the dialog is important &mdash; so you can share context. You have to push it, and help manage what the AI knows.</p>

      <p>It's also important to know what the AI is and isn't good at, or what's easy versus hard for it. You might not notice until you ask. If you ask Claude to create or read a Word doc, it'll do it &mdash; but it's actually difficult. Maybe there's an easier format that works the same for you but is quicker for Claude to handle.</p>
    </div>

    <hr class="separator">

    <div class="section">
      <h2>Set it up to help you</h2>

      <p>In <a href="https://docs.anthropic.com/en/docs/claude-code" target="_blank" rel="noopener">Claude Code</a> &mdash; the CLI tool I use for most of my work &mdash; Claude has a main memory file, other memory files, and skills. These determine how Claude operates and what it can do out of the box in any session. To make Claude really useful, these have to be managed well. It's hard for any AI to know what should be remembered from any session &mdash; you need judgment on that, and should actively tell it what's important to remember.</p>

      <p>If Claude makes a mistake, tell it to update its memory files. If you teach Claude how to do something kind of novel that you want to repeat or know about in future sessions &mdash; like using a new service or tool or process &mdash; you should ask Claude to turn it into a skill. For example, I have image and video generation skills that Claude can use whenever I need them.</p>

      <p>And finally, the AI doesn't have much volition of its own. If you want it to be genuinely proactive, you have to do the work to set it up that way.</p>
    </div>

    <hr class="separator">

    <div class="section">
      <h2>What I haven't covered yet</h2>

      <p>There's a lot more to say. Practical memory management. The weird edge cases and failure modes of different models. <a href="https://www.anthropic.com/research/disempowerment-patterns" target="_blank" rel="noopener">User disempowerment</a> &mdash; the risk that we let AI do our thinking for us and lose the ability to think well on our own. Responsible usage. How to evaluate what's working and what isn't. I'll get to it.</p>
    </div>

    <div class="footer">
      <p>This is a living document. Last updated February 2026.</p>
    </div>
  </div>
</body>
</html>
